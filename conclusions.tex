\chapter{Conclusions}\label{chap:conclusions}

Science is typically reductionist \cite{strogatzsyncbook}. We break a hard problem into smaller parts that are easier to understand separately. We have achieved tremendous success with this effort, but we have not solved everything; indeed, we have found out that putting everything back together can be quite complicated: the interactions between the parts can generate complex behavior that is not present in any one of the parts alone.
The field of complex systems arose from the need to understand this \textit{emergent phenomena} - to (re)construct the full system's behavior from knowledge of its parts. In the case of networked systems, studied in this thesis, the challenge can be phrased as the need to understand how the whole system's behavior arises from the coupling between the units. A major challenge still today is to develop tools that allows us to characterize and understand complicated emergent behavior.

One such complicated behavior in networks is the coexistence of multiple stable solutions to the same equations with the same parameters - \textit{multistability}! How do these solutions come about, where they are situated and how they are separated in state space - these are all questions under active research \cite{feudel2008complex, pisarchik2022multistability, zhang2021basins}. 

Some of these stable solutions may correspond to synchronized regimes, which brings into light another important phenomenon: \textit{synchronization}. Here again the field of complex systems has to contend with another problem: how individually distinct units can cooperate together and start to operate in unison, in a beautiful example of an emergent phenomenon. The study of synchronization - both frequency and phase synchronization - also has important practical motivations, for instance in the study of power grids. In power grids, and other complex networks, understanding the robustness of solutions, in particular of synchronized solutions has been an object of active research. 

% ------------------------------- malleability ------------------------------- %
Combining these two research areas, Chapter \ref{chap:malleability} investigated the robustness of solutions in a complex network of Kuramoto oscillators, a paradigmatic model for studies on synchronization phenomena and complex networks in general. The idea was to investigate how the network behaves - how the solutions change - when we alter the parameter of a single unit in the network. We found that the \textit{dynamical malleability} of the network depends on how strongly coupled the units are, and the topology of the connections. 
Roughly, we showed that for very weak coupling strength the individual tendencies of the oscillators win and most of them oscillate incoherently. For sufficiently strong coupling, most of the oscillators become phase locked - they oscillate at the same frequency. This is the same behavior as in all-to-all networks (see \secref{method:sec:kuramoto}). The spatial pattern of the phases, which we can measure via the degree of phase synchronization, was then determined by the topology. For several of the coexisting attractors, including the most phase synchronized attractor, the following tendency was observed: networks dominated by short-range connections tend to have attractors with short-range patterns (phase desynchronized), while networks dominated by long-range connections tend to have attractors with long-range patterns (phase synchronized). 
In parameter space, phase synchronization in these networks lives in the region of sufficiently high coupling strength and number of long-range connections. Changing the parameters toward this region therefore makes the system undergo a transition to phase synchronization. We showed that precisely during this transition their dynamical malleability increases considerably. To the point that changing a single unit radically alters the pattern of phases in the network, potentially changing it from phase synchronized to phase desynchronized. 

The mechanism for this dynamical malleability is two-fold. First, it is related to \textit{increased sample-to-sample fluctuations} near a phase transition \cite{hong2007entrainment, hong2007finitesizescalingpre}. This mechanism does not require multistability. In fact, suppose the systems have a single attractor, like the randomly connected networks. Each change to a parameter of a unit leads to a different dynamical system, which may have a different attractor. In particular, the transition to phase synchronization of this attractor may occur at different coupling strength values, earlier or later compared to the system before the change. If we enact this change but keep the coupling strength fixed, we switch to an attractor that has a smaller or larger value of phase synchronization - this is the fluctuation from one sample to another. If the systems have multiple attractors, this effect is still there, but there is the added possibility of switching to other attractors, which might be even more different. The \textit{multistability} increases the possible fluctuations that may occur. This explains our observation that for Watts-Strogatz networks the malleability and multistability seem to go hand in hand. It also explains why these networks have a considerably larger malleability than the distance-dependent networks, which do not seem to be multistable.

An important concept in the area of complex systems is that of global stability, typically taken to mean the relative size of the basin of attraction of each attractor. In this view, attractors whose basins occupy larger regions of state space are more globally stable \cite{menck2013how}. The rough idea is that trajectories on attractors with bigger basins of attraction are more likely to require bigger perturbations in order to be kicked across the basin boundary and into another attractor. This is not necessarily the case, however, since the situation depends on the geometry of the basin of attraction \cite{krakovska2023resilience}, but it highlights the importance of studying perturbations applied to the state of a system. In general, more attractors means they are sharing state space more and therefore the global stability is smaller, meaning the system is less robust (or less resilient, depending on terminology \cite{krakovska2023resilience}). In this work we show that multistability affects the robustness of the system in another way: by affecting its malleability. So not only is it dangerous to kick the state of the system, it is also dangerous to change its parameters - even the parameter of one single unit!

Another important observation was the study of how malleability, and multistability, depend on the topology of the system. Topologies that put the systems in the vicinity of a transition to phase synchronization, which were in the small-world range, made it very malleable. An important question that is left for future work is why these specific topologies lead to a higher number of attractors - which properties do they possess that lead to the emergence of the attractors, compared to, say, the random topologies, which do not induce multistability? The distance-dependent networks also do not seem to be multistable, a factor that would also be interesting to investigate. 

A related question is about the generality of these results. Malleability due to sample-to-sample fluctuations is very common, being extensively described in statistical physics literature \cite{sornette2006critical}. We also described it initially in a network of spiking neurons \cite{budzinski2020synchronization}, and observed it in the Kuramoto model under different topologies of distributions of the natural frequency, and under other models, such as a simple model of excitable cells. We believe that the multistability results will also generalize somehow - supported by the available evidence from other works - but this is also object of future research. Understanding better the mechanisms generating the multistability will also help answer this.

% ------------------------------ excitable units ----------------------------- %
In a similar vein, we also investigated in \chapref{chap:multistability} how multistability emerges when excitable neurons are coupled diffusively. Excitability in the individual units here occurs due to the presence of a saddle and an unstable equilibrium in state space, which force part of the trajectories to go around on a long excursion before eventually converging to the stable equilibrium. This region where trajectories go through is called the \textit{excitability region}. We showed that the coupling can trap trajectories in this excitability region by repeatedly reinjecting them there. This mechanism underlies all the emergent attractors we observed, even though they arise due to different bifurcations: saddle-node of limit cycles and homoclinic. For two units, it can create three coexisting periodic attractors, and can also create a quasiperiodic attractor. For more units, it can create a larger number of attractors, including potentially a chaotic attractor. Based on the trapping mechanism and preliminary results, we conjecture that the topology of the networks plays a key role in dictating which attractors emerge, and how many. This could be very similar to Kuramoto networks, and a more in-depth comparison is definitely warranted. It would be very interesting in the future to explore how exactly the size and topology of the networks control the emerging attractors. 

In this initial work we decided to focus mainly on the pure dynamics of the system, so we showed most of the results in the case where the coupling is applied to both the $x$ and $y$ directions of the system. In some models, such as ecological models - where the diffusive term would model a migration of species - this might be very sound. For the neuronal case, however, only the $x$-coupling is biophysically sound. Motivated by this fact, we also investigated how the attractors change when the coupling is applied to only one variable. Interestingly, the mechanism is still present, but the two main types of attractors we observed split up when the coupling is split. The exclusive $x$-coupling leads to the attractor with two units trapped in the excitability region (LA-LA); the exclusive $y$-coupling leads to the one with only one unit in the excitability region (LA-SA or SA-LA). We confirmed this with a bifurcation analysis and also qualitatively explained it based on the geometry of the attractors and the trapping mechanism. This is important in terms of potential applications. First, it means that adding a gap junction between two otherwise silent neurons could make them bistable, with the possibility of periodic or even quasiperiodic spiking. In fact, there is some evidence that this seems to occur in neurons coupled under gap junctions in the motor cortex of fruit flies \cite{hurkey2023gap}. It is also interesting in the ecological direction, if we consider that only some species in an ecological niche might be migrating between patches.

Furthermore, we focused for simplicity on the excitable case, where the trapping mechanism creating the attractors is more easily seen. But attractors still emerge similarly in a bistable regime, where the stable equilibrium coexists with a stable limit cycle. We can achieve this by changing the input current $I$ of the model. A difference in this case is that the uncoupled neuron already has an oscillating attractor. Therefore, when they are diffusively coupled they can also synchronize together in this oscillating attractor. This system thus has the possibility of achieving full synchronization on a periodic attractor. In this case, one could reframe the study in terms of the stability, global and linear, of the synchronized state, and how the coupling might create new attractors and thus reduce the relative size of the basin of the synchronized attractor.

We initially arrived at this problem when trying to understand the synchronization behavior of a network of bursting neurons \cite{rossi2021phase}. The degree of phase synchronization in that system changes nonmonotonically as a function of the coupling strength: increasing the coupling initially increases the phase synchronization, then actually decreases it in a certain region, before increasing it again for very strong coupling. This is also reminiscent of a behavior observed in networks with chaotic saddles in \refref{medeiros2018boundaries}. We also studied a network of bursting neurons following another model, and found that a chaotic saddle was important there but also a slow region of system's limit cycle was related to the multistability that emerged. From the work on excitable neurons, we understand that slowness can help generate attractors, at least for the reinjection mechanism we observed. It would be interesting in the future to go back and finish the initial studies. 

% ------------------------------- Attractors.jl ------------------------------ %
When working on a project, I believe it is not an uncommon feeling to find an interesting paper, try to replicate its results and not quite manage. Then, to look at the source code that the authors hopefully provided, and to be underwhelmed. While working on a paper, it is often the case that people might want to spend as little time as possible implementing the algorithms they need, leading usually to confusing code, which might not be as efficient as it could, and not as well-tested - and thus, more susceptible to errors. 
One solution to this is to create a unified library that implements efficient code, tests and documents it. And to make it open-source, to share it with the whole community. Then, anyone can scrutinize the code, find improvements and test it further. Also, more importantly, everyone can use it. This saves implementation time, potentially run times due to improved code efficiency, and also re-implementation time for poor students aiming to replicate papers. This is the philosophy of the dynamical systems library \cite{datseris2018dynamical}, started by Dr. George Datseris, written in the Julia programming language. With this idea in mind, we also collaborated to implement algorithms related to finding attractors and their basins. In particular, I worked on the algorithm used in the two multistability works in this thesis. It is a brute-force algorithm that integrates trajectories, converts them into vectors of features, and selects attractors as unique groups of features \cite{gelbrecht2020monte, stender2021bstab, datseris2023framework}. Together with Prof. Alexander Wagemakers, we also implemented an algorithm that applies attractor-finding algorithms across a parameter range, in a continuation manner. The result of this work was the \textit{Attractors.jl} package, also co-developed by more collaborators, and a publication describing this novel algorithm and improvements to previous literature \cite{datseris2023framework}.


% ------------------------------- metastability ------------------------------ %
% \subsubsection*{Metastability}
So far on the study of dynamical systems we have mostly focused on attractors. The motivation for this is that attractors represent a system's long-term dynamics: after some \textit{transient} time, trajectories converge to attractors. There is, however, a key assumption here: that the period of time during which we observe the system $T_\mathrm{obs}$ is longer than the convergence time $T_\mathrm{conv}$ to the attractor. It is a matter of time-scales: of the observation versus the relaxation to the attractor. Whether this can be guaranteed or not depends on the application. In power grids, for instance, one is generally interested in the long-term dynamics of the system. In the brain, however, changes may be occurring too fast, and there may not be enough time to wait for convergence to an attractor. The time-scales can also vary within the same system: as we saw in the excitable units, trajectories starting on one side of the state space converge rapidly to the attractor, whereas trajectories starting on the excitability region spend a relatively long time performing an excursion in space before reaching the attractor. This problem is made more complicated due to the fact that there are many mechanisms that can generate long - potentially arbitrarily long - transients. An example is chaotic saddles, wherein trajectories can stay indefinitely long \cite{lai2009transient}. Therefore, the behavior that is actually observed in some studies may be a transient. 
Moreover, some of these long-lived transients occur inside attractors. One example can be seen in ghost states inside chaotic attractors - such as for the Logistic map or the Lorenz system - where the trajectories switch between clearly chaotic and seemingly periodic dynamics (cf. \chapref{chap:metastability}). Another example is the stable heteroclinic cycle: the cycle as a whole can be an attractor, but trajectories on it switch between the neighborhoods of saddle-points, describing sequences of metastable regimes. Yet another example is crawl-by motion, in which a limit cycle is in the proximity of a saddle-point. The region near the saddle-point may have very slow dynamics, and trajectories on the cycle take a long time to pass through (crawl-by) this region \cite{hastings2018transient}. These examples illustrate the intricate relation between multistability - and attractors - and metastability. 

Transients can play important roles. A specific example that illustrates the role of sequences of transient states is the Turing machine, the paradigmatic model for \textit{computations} \cite{turing1936on, computabilitybook}. It is a simple finite state machine with a head that stores a certain state and can read, write, and move along a tape. The tape is subdivided into cells containing symbols (e.g., 0's and 1's). The head represents a modern computer's central processing unit, while the tape represents the memory. Accordingly, the head follows a set of instructions that take the current state, currently read symbol on the tape and outputs the new state, the new symbol it writes on the tape, and the direction it moves. Computations are done by traversing a sequence of such state-symbol combinations. The machine may run forever - it is said to not halt -, in which case the computation is not completed. If the machine does halt, the computation is finished. From this point of view, therefore, the computation is only complete once the machine terminates the previous sequence of states. This sequence can therefore be seen as a type of transient behavior, which is crucial for the computation performed by the machine. This remark is not just an analogy - dynamical systems can be constructed that implement Turing machines \cite{postlethwaite2024a}.

More concretely, in the brain, transients have been shown to play important roles \cite{ashwin2005when, mazor2005transient}. There is a plethora of observations showing neural activity going through sequences of distinct states, which are all therefore transient \cite{tognoli2014metastable, brinkman2022metastable}. In several cases, these states are long-lived (i.e., metastable). Understanding the exact roles that \textit{metastable regimes} play in neural circuits is crucial to understanding how they perform computations, a central question in neuroscience and also artificial intelligence \cite{koch2024biological, vyas2020computation, sussillo2013opening}. Recent work, based on theoretical and experimental results, has shown that ghosts of saddle-node equilibria, which generate long transients, are a particularly important mechanism \cite{koch2024biological, koch2024ghost, nandan2022cells, mazor2005transient}. It is expected, however, that other mechanisms are also present in circuits. For instance, a wide literature in neuroscience uses attractors to perform computations, and adds external perturbations to induce changes between regimes \cite{driscoll2024flexible, sussillo2013opening, brinkman2022metastable, laje2013robust}. It will be important in the future to contrast these two ideas to see the actual roles played by each of them.

To better understand the role of transients on computations in neural circuits, it is therefore important to have both an in-depth as well as a general understanding of metastable dynamics. Under this logic we developed a \textit{general conceptual framework} for metastability, collecting and refining ideas from the neuroscience and dynamical systems literatures. As seen in \chapref{chap:metastability}, we proposed that the main concept behind metastability is that of long-lived transients, and showed many dynamical mechanisms capable of generating it. In the future, one can use this framework to actively compare the different mechanisms, with a view towards experiments - both biological as well as \text{in silico}, looking to understand how networks perform computations \cite{koch2024biological}.

Besides the metastable regimes themselves, perhaps the actual \textit{sequences} play an important role. This is the case in the Turing machine, but there is also evidence in biological networks. An important example, already mentioned in the Introduction and in \chapref{chap:metastability}, showed in a series of works that sequences of metastable regimes are elicited when mice are fed tastants \cite{jones2007natural}. The sequence of regimes is unique to each tastant, suggesting they play an active role in encoding the stimuli \cite{lacamera2019cortical}. Sequences of metastable regimes have been linked to computations in other experiments also \cite{mazor2005transient, lacamera2019cortical, driscoll2024flexible}. In this case, a useful concept coming from dynamical systems theory is that of excitable networks by Ashwin and Postlethwaite \cite{ashwin2021excitable, ashwin2024network}. They developed methods that allow one to construct systems with prescribed connections between equilibria states. These connections may be spontaneously activated (as in connected ghosts) or via a perturbation (by perturbing across the basin boundary). This is an example of how the theory of dynamical systems is offering many tools and mechanisms that can be used to model and better understand how circuits are actually solving tasks and performing computations. This is an exciting area for future research.

% ------------------------------------ END ----------------------------------- %
Taking everything together, the field of complex systems is under intense research, with lots of us aiming to develop theory and tools to understand emergent dynamical phenomena like synchronization, the coexistence of multiple long-term solutions and the (transient) path to them. I believe that during my PhD we managed to provide some timely contributions in these directions, but there is still much to be done - with applications being very significant in biology, technology and even climate. I am very excited to help put all these pieces together.



