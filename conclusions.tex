\chapter{Conclusions}\label{chap:conclusions}

Science is typically reductionist \cite{strogatzsyncbook}. We break a hard problem into smaller parts that are easier to understand separately. We have achieved tremendous success with this effort. But we have not solved everything; indeed, we have found out that putting everything back together can be quite complicated: interactions between even simple units can generate complicated behavior that is not present in any one of the units by themselves.
The field of complex systems arose from the need to understand emergent phenomena - to (re)construct the full system's behavior from knowledge of its parts. A major challenge still today is to develop tools that allows us to characterize and figure out this complicated emergent behavior.

One such complicated behavior is the coexistence of multiple stable solutions to the same equations with the same parameters - \textit{multistability}! How do these solutions come about, where they are situated, how they are they separated in state space - these are all questions under active research \cite{feudel2008complex, pisarchik2022multistability, zhang2021basins}. 

Some of these stable solutions may correspond to synchronized regimes, which brings into light another important phenomenon: \textit{synchronization}. Here again the field of complex systems has to contend with another problem: how individually distinct units can cooperate together and start to operate in unison, in a beautiful example of an emergent phenomenon. The study of synchronization - both frequency and phase synchronization - also has important practical motivations, for instance in the study of power grids. In power grids, and other complex networks, understanding the robustness of solutions, in particular of synchronized solutions has been an object of active research. 

% ------------------------------- malleability ------------------------------- %
Combining these two research areas, Chapter \ref{chap:malleability} investigated the robustness of solutions in a complex network of Kuramoto oscillators, a paradigmatic model for studies on synchronization phenomena and complex networks in general. The idea was to investigate how the network behaves - how the solutions change - when we alter the parameter of a single unit in the network. We found that the \textit{dynamical malleability} of the network depends on how strongly coupled the units are, and the topology of the connections. 
Roughly, we showed that for very weak coupling strength the individual tendencies of the oscillators win and most of them oscillate incoherently. For sufficiently strong coupling, most of the oscillators become phase locked - they oscillate at the same frequency. This is the same behavior as in all-to-all networks (see Sec.\ref{method:sec:kuramoto}). The spatial pattern of the phases, which we can measure via the degree of phase synchronization, was then determined by the topology. Networks dominated by short-range connections tended to have short-range patterns (phase desynchronized), while networks dominated by long-range connections tended to have long-range patterns (phase synchronized). These networks typically have multiple attractors coexisting, but most of the attractors, including the most synchronized attractor, follow this tendency.
In parameter space, phase synchronization in these networks lives in the region of sufficiently high coupling strength and number of long-range connections. Changing the parameters toward this region therefore makes the system undergo a transition to phase synchronization. We showed that precisely during this transition their dynamical malleability increases considerably. To the point that changing a single unit radically alters the pattern of phases in the network, potentially changing it from phase synchronized to phase desynchronized. 

The mechanism for this dynamical malleability is two-fold. First, it is related to increased sample-to-sample fluctuations near a phase transition \cite{hong2007entrainment, hong2007finitesizescalingpre}. This mechanism does not require multistability. In fact, suppose the systems have a single attractor, like the randomly connected networks. Each change to a parameter of a unit leads to a different dynamical system, which may have a different attractor. In particular, the transition to phase synchronization of this attractor may occur at different coupling strength values, earlier or later compared to the system before the change. If we enact this change but keep the coupling strength fixed, we switch to an attractor that has a smaller or larger value of phase synchronization - this is the fluctuation from one sample to another. If the systems have multiple attractors, this effect is still there, but there is the added possibility of switching to other attractors, which might be even more different. The multistability increases the possible fluctuations that may occur. This explains our observation that for Watts-Strogatz networks the malleability and multistability seem to go hand in hand. It also explains why these networks have a considerably larger malleability than the distance-dependent networks, which do not seem to be multistable.

An important new concept in the area of complex systems is that of global stability, typically taken to mean basically the relative size of the basin of attraction of each attractor - attractors that occupy larger regions of state space are more globally stable, in this view \cite{menck2013how}. Considering a trajectory on an attractor, bigger basins of attraction mean that bigger perturbations are on average needed in order to kick the trajectory across the basin boundary and into another attractor. This is, of course, a simplification \cite{krakovska2023resilience}, but it highlights the importance given to studying perturbations applied to the state a system. And, in general, more attractors means they are sharing state space more and therefore the global stability is smaller, meaning the system is less robust (or less resilient, depending on terminology \cite{krakovska2023resilience}). In this work we show that multistability affets the robustness of the system in another way: by affecting its malleability. So not only is it dangerous to kick the state of the system, it is also dangerous to change its parameters - even the parameter of one single unit!

Another important observation was the study of how malleability, and multistability, depend on the topology of the system. Topologies that put the systems in the vinicity of a transition to phase synchronization, which were in the small-world range, made it very malleable. An important question that is left for future work is why these specific topologies lead to a higher number of attractors - which properties do they possess that lead to the emergence of the attractors, compared to, say, the random topologies, which do not induce multistability? The distance-dependent networks also do not seem to be multistable, a factor that would also be interesting to investigate. 

A related question is about the generality of these results. Malleability due to STS fluctuations seems to be quite general, being extensibly described in statistical physics literature \cite{sornette2006critical}. We also described it initially in a network of spiking neurons \cite{budzinski2020synchronization}, and observed it in the Kuramoto model under different topologies of distributions of the natural frequency, and under other models, such as a simple model of excitable cells. We are confident the multistability results will somehow also generalize - supported by the available evidence from other works - but this is also object of future research. Understanding better the mechanisms generating this multistability will also help answer this.

% ------------------------------ excitable units ----------------------------- %
In a similar vein, we also investigated how multistability emerges when excitable neurons are coupled diffusively. Excitability in the individual units here occurs due to the presence of a saddle and an unstable equilibrium in state space, which force part of the trajectories to go around on a long excursion before eventually converging to the stable equilibrium. These region where trajectories go through is called the excitability region. We showed that the coupling can trap trajectories in this excitability region by repeatedly reinjecting them there. This mechanism underlies all the emergent attractors we observed, even though they arise due to different bifurcations: saddle-node of limit cycles and homoclinic. For two units, it can create three coexisting periodic attractors, and can also create a quasiperiodic attractor. For more units, it can create a larger number of attractors, with potentially chaos. Based on the trapping mechanism and preliminary results, we conjecture that the topology of the networks plays a key role in dictating which attrators emerge, and how many. This could be very similar to Kuramoto networks, and a more in-depth comparison is definitely warranted. It would be very interesting in the future to explore how exactly the size and topology of the networks control the emerging attractors. 

In this initial work we decided to focus mainly on the pure dynamics of the system, so we showed most of the results in the case where the coupling is applied to both the $x$ and $y$ directions of the system. In some models, such as ecological models - where the diffusive term would model a migration of species - this might be very sound. For the neuronal case, however, only the $x$-coupling is biophysically sound. Motivated by this fact, we also investigated how the attractors change when the coupling is applied to only one variable. Interestingly, the mechanism is still present, but the two main types of attractors we observed - with two units trapped in the excitability region or with just one - split up when we split the coupling. The exclusive $x$-coupling got former; the exclusive $y$-coupling got the latter. We confirmed this with a bifurcation analysis and also qualitatively explained it based on the geometry of the attractors and the trapping mechanism. This is important in terms of potential applications. First, it would adding a gap junction between two otherwise silent neurons could make them bistable, with the possibility of periodic or even quasiperiodic spiking. In fact, there is some evidence that this seems to occur in neurons coupled under gap junctions in the motor cortex of fruit flies \cite{hurkey2023gap}. It is also interesting in the ecological direction, if we consider that only some species in an ecological niche might be migrating between patches.

Furthermore, we focus for simplicity on the excitable case, where the trapping mechanisms creating the attractors is more easily seen. But attractors still emerge similarly in a bistable regime, where the stable equilibrium coexists with a stable limit cycle. We can achieve this by changing the input current $I$ of the model. A difference in this case is that the uncoupled neuron already has an oscillating attractor. Therefore, when they are diffusively coupled they can also synchronize together in this oscillating attractor. This system thus has the possibility of achieving full synchronization on a periodic attractor. In this case, one could reframe the study in terms of the stability, global and linear, of the synchronized state. How the coupling might create new attractors and thus reduce the relative size of the basin of the synchronized attractor.

We initially arrived at this this problem when trying to understand the synchronization behavior of a network of bursting neurons \cite{rossi2021phase}. The degree of phase synchronization in this system changes nonmonotonically as a function of the coupling strength: increasing the coupling initially increases the phase synchronization, then actually decreases it in a certain region, before increasing it again for very strong coupling. This is also reminiscent of a behavior observed in networks with chaotic saddles in \cite{medeiros2018boundaries}. We also studied a network of bursting neurons following another model, and found there that a chaotic saddle was important but also a slow region of system's limit cycle was related to the multistability that emerged there. From the work on excitable neurons, we understand that slowness can help generate attractors, at least for the reinjectin mechanism we observed. It would be interesting in the future to go back to the initial studies. 

% ------------------------------- Attractors.jl ------------------------------ %
I believe it is not an uncommon feeling to find an interesting paper, try to replicate its results and not quite manage. Then, to look at the source code that the authors hopefully provided, and to be underwhelmed. While working on a paper, experience tells me that people usually want spend as little time as possible implementing the algorithms they need, leading usually to confusing code, which might not be as efficient as it could, and not as well-tested - and thus, more susceptible to errors. 
One solution to this is to create a unified library that implements efficient code, tests and documents it. And to make it open-source, to share it with the whole community. Then, anyone can scrutinize the code, find improvements and test it further. Also, more importantly, everyone can use it. This saves implementation time, potentially run times due improved code efficiency, and also re-implementation time for poor students aiming to replicate papers. This is the philosophy of the dynamical systems library \cite{datseris2018dynamical}, started by Dr. George Datseris, written in the Julia programming language. With this idea in mind, we also collaborated to implement algorithms related to finding attractors and their basins. In particular, I worked the algorithm used in the two multistability works in this thesis. It is a brute-force algorithm that integrates trajectories, converts them into vectors of features, and selects attractors as unique groups of features \cite{gelbrecht2020monte, stender2021bstab, datseris2023framework}. Together with Prof. Alexander Wagemakers, we also implemented an algorithm that applies attractor-finding algorithms across a parameter range, in a continuation manner. The result of this work was the Attractors.jl package, also co-developed by more collaborators, and a publication describing this novel algorithm and improvements to previous literature \cite{datseris2023framework}.


% ------------------------------- metastability ------------------------------ %
So far on the study of dynamical systems we have mostly focused on their long-term dynamics, looking at their attractors. The motivation for this is that attractors represent a system's long-term dynamics: after some transient time, one just observes the attractors. There is, however, a key assumption here: that the period of time during which we observe the system $T_\mathrm{obs}$ is longer than the convergence time $T_\mathrm{conv}$ to the attractor. It is a matter of time scales: of the observation versus the relaxation to the attractor. Whether this can be guaranteed or not depends on the application. In power grids, for instance, one is generally interested in the long-term dynamics of the system. In the brain, however, changes may be occurring too fast, and there may not be enough time to wait for convergence to an attractor. The time scales can also vary within the same system: as we saw in the excitable units, trajectories starting on one side of the state space converge rapidly to the attractor, whereas trajectories starting on the excitability region spend a relatively long time performing an excursion in space before reaching the attractor.

This problem is made more complicated due to the fact that there are many mechanisms that can generate long - sometimes arbitrarily long - transients. An example is chaotic saddles, wherein trajectories can stay indefinitely long \cite{}.
Therefore, in some case studies the behavior that is actually observed may be a transient. 

There is a lot of evidence towards pointing that transients can indeed play important roles. One example that illustrates this is the Turing machine. XX. computation is done on the transient. 

In the brain, transients have been shown to play important roles. plethora of observations in the literature. important works, lots of definitions, some proposals of mechanisms. lack of conceptual framework. 


% Besides allowing for a clearer understanding of works, it can also help in future studies looking into the mechanisms behind observations and into why metastability is so ubiquitous - is there any advantage of using long transients for doing computations, as opposed to using attractors \cite{koch2024XX}?  
despite the initial motivation being to provide a nice definition, the work outgrew this and became a conceptual framework to think about metatsability, in what we believe is an important step toward a general understanding of transient dynamics. 

computations, advantages

% ------------------------------------ END ----------------------------------- %
complex networks, topology, sync (hearken back to daad proposal)
toward complete understanding of the behavior of the system, on the attractors, and before 
trying to put all those pieces together

