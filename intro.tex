\chapter{Introduction}
Consider the unfortunate situation of falling down a mountain. Subject to the inexorable effect of gravity and friction, the hiker will roll downhill until they reach a certain valley, a spot at which they will finally terminate their unlucky dynamics. This final state is called an attractor, and the preceding rolling period is called a transient. Now, consider a landscape like the one in \figref{fig:intro:landscape}A. The mountain has several valleys, separated by peaks. An example of this separation is shown in \figref{fig:intro:landscape}B. Consider then the even more unfortunate situation of \textit{two} people falling down a mountain. If they start very close together, on the same side of a peak, they will fall down to the same valley. If, however, they were separated by a peak when the fall started, then they will fall into distinct valleys. This is shown by the green and red trajectories in Figs.~\ref{fig:intro:landscape}A-B. Again, each valley is an attractor. Which attractor the person falls into depends on their initial condition. If we consider this particular example to be a mechanical system with inertia, the initial condition corresponds to the person's initial position and velocity. In general, all initial conditions that lead to the same attractor form a set called the basin of attraction of that attractor. All the red trajectories in \figref{fig:intro:landscape}A belong to the same basin. Trajectories are typically separated by peaks in the landscape (green and red of \figref{fig:intro:landscape}B), so the peaks usually form the boundaries between basins of attraction.
%
\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{intro/landscape.png}
    \caption{\textbf{Landscape with valleys and peaks constitutes an example of multistability for an unfortunate falling person.} Panels A and B respectively show a 3D and 2D example of a landscape, with red trajectories converging onto the same attracting region, and green trajectories, which start next to the red trajectories but on the other side of the peak, converge onto other attracting regions.}
    \label{fig:intro:landscape}
\end{figure}

The example of the hiking disaster serves as a good introduction to the notion of \textit{multistability} - the simultaneous coexistence of different ending states, different attractors, in a dynamical system with constant parameters (notice that the mountain landscape does not change in time in the example!). This phenomenon is present in a wide variety of notable systems, with important real-world consequences \cite{feudel2008complex, pisarchik2022multistability, pisarchik2014control}. In biology, multistability can explain how genetically identical cells can exist in multiple metabolically distinct stable states \cite{zhu2022synthetic, regan2012dynamical}. Similarly, there has been evidence, and models, suggesting that multistability in the gut microbiome can explain microbiome shifts, which are changes in the composition of the microbiome in the gut \cite{khazaei2022metabolic}. On a technological side, power grids - networks of connected generators and consumers of electrical energy - need to operate on an attractor in which all units have their frequencies of oscillation synchronized in the 50-60Hz range \cite{hellmann2020network}. Multistability in the grids can be dangerous, as perturbations can switch the system out of the operating state, potentially leading to blackouts. Studies on models try to look for conditions that make the desired state as stable as possible \cite{hellmann2020network, halekotte2021transient}. Multistability can also be a powerful mechanism in brain dynamics. Some models for long-term memory consider that each memory corresponds to an attractor in the system \cite{wilson1972excitatory, foss1996multistability}, and some models of large-scale brain dynamics exhibit multistability \cite{golos2015multistability}. There are many more examples of multistability, such as in artificial neural networks \cite{flynn2024exploring}, models for ice sheets \cite{robinson2012multistability}, mechanical systems \cite{feudel1998dynamical}, and in tissue repair \cite{adler2020principles}.

The examples in neural networks and power grids in particular highlight the ubiquitous presence of multistability in networked systems - systems formed by the interactions of smaller subunits, such as neurons or electric generators.  Another phenomenon in networks that can coexist with multistability is \textit{synchronization} \cite{pikovsky2001synchronization, boccaletti2018synchronization}. Often the interaction between units of a network can cause them to adjust their individual rhythms toward a collective motion, in which their activity becomes similar in some sense \cite{pikovsky2001synchronization}. In this case, the network is said to be synchronized. There are many types of synchronization, which can be defined based on what ``similar'' and ``activity'' are taken to mean. Three important properties that characterize the state of an oscillation at a certain time are the amplitude, phase, and angular frequency at that time. Roughly, the amplitude describes how high or how low the oscillation itself is; the phase describes the current position of the oscillation along its oscillation cycle; and the angular frequency describes how fast the phase itself is moving \cite{pikovsky2001synchronization}.
In many applications, the amplitudes of the oscillations turn out to not play a major role \cite{pikovsky2001synchronization, arenas2008synchronization}. For instance, if the coupling between the units is weak enough, the amplitudes of their oscillations are not particularly altered, but the phases are \cite{pikovsky2001synchronization, strogatz2000from}. When units adjust their individual frequencies to the same value, i.e. when they spontaneously lock into a common frequency, they become frequency synchronized (also called phase-locked) \cite{pikovsky2001synchronization, strogatz2000from}. One technically relevant example has been mentioned before for power grids, in which all the units must have their frequencies synchronized at the same value, such as 50 Hz \cite{hellmann2020network}. In some cases, the phases of the oscillations may also converge onto a common value. Then, the units are phase-locked and their phases converge to similar values. This is called \textit{phase synchronization}. It has been proposed as an important mechanism for communication and exchange of information between brain circuits \cite{singer1999neuronal, fries2015rhythms, womelsdorf2007the}. Interestingly, lack of phase synchrony can also play an important role, e.g. in the flight pattern of fruit flies \cite{hurkey2023gap}. 

The real-world relevance of such systems has stimulated a lot of research into their dynamics \cite{feudel2008complex}. An approach taken by several works has been to study simple models that capture some essential properties of real world systems. A particularly important example, which has become paradigmatic in the synchronization literature, is that of Kuramoto oscillators (see \secref{method:sec:kuramoto}). They constitute quite a beautiful example of how units with very simple dynamics can generate complex behavior when interacting together. Each unit in the model is described by a phase (angle) variable that by itself just varies linearly according to its own natural frequency. The interesting dynamics comes from the nonlinear coupling, done via the sin of the phase difference between coupled units, cf. \eqnref{eq:kuramoto-general}. The model is simple enough to allow for analytic treatment but still complex enough to show relevant dynamics \cite{strogatz2000from, acebron2005kuramoto}. In particular, it displays a continuous phase transition from desynchronization to frequency and phase synchronization as the strength of the inter-unit coupling is increased. Roughly, if the natural frequencies are spread too widely compared to the coupling between them, the units oscillate incoherently; if instead the coupling becomes large enough, the units start to oscillate with the same frequency - they become phase-locked. As the coupling increases, the phases also become more clustered together, although complete phase synchronization does not occur. 

The Kuramoto model is also generic in the sense that it can be derived as an approximation of general limit cycle oscillators under weak coupling \cite{boccaletti2018synchronization}. In this case, one considers units that oscillate on a periodic orbit. If the coupling between the units is weak enough, the amplitude of their oscillation is not significantly affected, only the phase along the limit cycle. Then, the interplay between the differences in frequency and the coupling determines the time evolution of the phases. The Kuramoto model is a somewhat more specific case of this phase reduction, in that one chooses a purely sinusoidal coupling \cite{strogatz2000from}. Still, the combination of simplicity and complexity leading to a synchronization transition, and this argument of genericity, incited a lot of research and inspired new concepts \cite{acebron2005kuramoto, rodrigues2016the, strogatz2000from}.

This also inspired us to translate results we had from spiking neural networks \cite{budzinski2020synchronization}. In those networks we described a phenomenon we called \textit{dynamical malleability}, the sensitivity of a whole network's dynamics to changes in parameters of single components, usually changes in parameters of single units. Similarly to the Kuramoto oscillator networks, the spiking neural networks we studied also present a transition to synchronization, in particular to phase synchronization, when the coupling strength is increased. They also present a transition to synchronization as the topology changes: as the connections in the system are changed from being restricted only to $k$-nearest neighbors to being randomly allocated, the neurons also start to synchronize their phases. Types of topologies are described in more detail in \secref{method:sec:network}. In the neighborhood of both of these transitions, we showed that the network's dynamical malleability increases considerably. As we see in \chapref{chap:malleability}, this phenomenology generalizes for Kuramoto networks with heterogeneous frequencies. In fact, it occurs very strongly: changing the parameter of a single unit can drastically alter the behavior of the whole network in a very sensitive manner \cite{rossi2022shifts}, which was until then not known. 

In the literature for Kuramoto oscillators, the phenomenon of dynamical malleability has been studied from the point of view of statistical mechanics \cite{hong2007entrainment, hong2007finitesizescalingpre}. Changing the parameters of a single unit leads to a different network, which is termed to be a different sample. In this case, one shows that the finite size of the networks leads only to an approximate phase transition, whose critical parameter varies depending on the sample. Therefore, in this case, one can show that \textit{sample-to-sample (STS) fluctuations} increase near a phase transition. These studies did not, however, look closely at the dynamics of these finite networks. One work that looks at this more closely for all-to-all topologies was \refref{peter2018transition}, where they propose that the kurtosis of the natural frequency distribution correlates with the critical coupling strength of the transition. Therefore, changing the frequency of the units changes the kurtosis and thus changes the critical coupling strength. However, they did not explore how this also interacted with more complex topologies. As we show in our work, their mechanism alone does not explain the malleability we describe: networks with shuffled natural frequencies have the same kurtosis but still can vary significantly. In our work, we therefore complement these studies by looking at the dynamics behind the malleable networks, and show that indeed the sample-to-sample fluctuations are a key effect leading to malleability. But we also show that another important effect comes from multistability, which is another behavior we analyzed. 

We looked at \textit{multistability} in the networks as a function of the coupling strength and topology, and showed the emergence of a large number of coexisting attractors at the transition to phase synchronization. This therefore means that the networks we studied are very sensitive to perturbations in the state variables (which can lead the system to switch to other attractors, due to multistability) and in the parameters (which can change the attractor considerably, due to malleability). This was another contribution from our work. Naturally, there have been studies on multistability in Kuramoto networks. In the case of heterogeneous frequencies, Tilles et al. studied multistability arising in nearest-neighbor rings \cite{tilles2011multistable}. In a related Kuramoto model, which has an inertial term, some studies have shown the coexistence of multiple attractors in random topologies \cite{gelbrecht2020monte}, and in power grid topologies \cite{hellmann2020network, halekotte2021transient}. Ref. \cite{potratzki2024synchronization} looks at how properties of power grid topologies relate to the dynamics of first-order Kuramoto models, but do not report multistability.

Multistability has been studied in detail for units with \textit{homogeneous frequencies} (which are then identical) and which are coupled in $k$-nearest-neighbor topologies. In this case, the network can be written as a gradient system, meaning its only attractors are equilibria, which are single points in state space (cf., Secs. \ref{method:linear-system}-\ref{method:nonlinear-I}). This considerably simplifies their study. The networks can have multiple stable equilibria, each being characterized by neighboring units having a fixed and constant phase relationship. These equilibria are called twisted states \cite{wiley2006the}, and their stability depends on the relationship between the number of nearest neighbors $k$ and the size $N$ of the network \cite{wiley2006the} - see \secref{method:sec:kuramoto:twisted} for more. For these networks, there have been studies looking at the effect of the topology \cite{townsend2020dense}, showing a minimum coupling strength that guarantees complete synchronization globally. Another important contribution has looked at the basins of attraction for these networks: Zhang and Strogatz have shown that the basins behave like octopuses - the head of the octopus contains the attractor, an equilibrium. The head is relatively small compared to the tentacles: most of the volume of the basins is not concentrated around the equilibrium, but spread around in tentacle-like structures in state space \cite{zhang2021basins}. 

In both the case of heterogeneous and of homogeneous frequencies, we are unaware of any systematic study on the emergence of multistability and effect of changing topology, in particular for first-order Kuramoto models. Our work serves also as a step in this direction, but more research is needed. 

In general, the mechanisms that give rise to multistability in networks are still not fully understood. In particular, during my PhD we started to study multistability in a network of bursting neurons coupled diffusively, looking to explain results from previous publications \cite{rossi2021phase}. The neurons, which follow the Hindmarsh-Rose equations \cite{hindmarsh1984a}, have individually a stable periodic orbit as an attractor. By changing parameters of the neurons, one can make a certain region of this periodic orbit very slow, but without going through a bifurcation. Preliminary results showed that multistability only emerges in the coupled networks when the neurons have this slow region. To better understand this, we looked at a simpler conductance-based neuronal model \cite{izhikevichbook} which also has regions of slow flow. We focused on the case when this model has excitable dynamics. The isolated neuron then has only one attractor, a stable equilibrium. And it also has two unstable equilibria, which force some trajectories to go on long excursions before converging to that attractor. These excursions are called excitations, and correspond to the neuron spiking. One of these unstable equilibria also slows down trajectories passing near it. By coupling two such neurons diffusively we show the emergence of different types of oscillating attractors, which can all coexist. We show the bifurcations giving rise to these attractors. Furthermore, we describe a qualitative mechanism for how they occur. The idea is that the diffusive coupling acts to repeatedly reinject the trajectories of each neuron into the region responsible for the excitations, thereby effectively \textit{trapping the trajectories in the previously transient region} - see \chapref{chap:multistability} for more. The slowness near one the equilibria plays an important role in this mechanism, which might help to explain the original problem we started on. For two units, it can happen that both are trapped in this excitability region, or just one is, generating in total three possible combinations. For more units, the number of possible combinations increases, and therefore so does the number of coexisting attractors. The emerging attractors are all oscillating, and can do so periodically, quasiperiodically, or chaotically - all despite the individual units having only equilibria!  
This mechanism is also a simple example of how coupling can interact with transients to generate attractors, an idea that has been studied in the literature under different circumstances. In particular, Medeiros et al. studied units which have a periodic attractor and a chaotic saddle, an unstable chaotic set, in their state space. They showed that diffusive coupling between them can counteract the divergence tendency near the chaotic saddle, effectively trapping the units in its neighborhood, and creating a chaotic attractor which coexists with the units' periodic attractor \cite{medeiros2018boundaries, medeiros2019state}. However, the authors did not observe multiple attractors emerging from the trapping in the chaotic saddle. Therefore, the coupled excitable neurons, with their trapping mechanism, constitutes a simple yet powerful mechanism for generating a rich multistability in networks, which had not been described previously in the literature, to our knowledge.

This line of investigation on multistability also contributes to the study of how oscillations arise in non-oscillating units interacting via diffusive coupling. As discussed in \chapref{chap:multistability}, this line of work has a rich history, with an early work by Smale showing that Hopf bifurcations can give rise to oscillations \cite{smale1976a} - see \secref{method:bifurcations} for bifurcations. Later works showed the possibility of chaos, and also the emergence of multistability in repulsive coupling. Our contribution in this case has been to show a rich multistability, with the possible coexistence of periodic, quasiperiodic, and also chaotic solutions - with repulsive or attracting coupling. 

These studies on multistability require efficient and reliable algorithms to identify the coexisting attractors of a system. To this end, I have contributed to creating \textit{Attractors.jl}, an open-source package in the Julia programming language that collects such algorithms. In particular, George Datseris and Alexander Wagemakers had already introduced an algorithm to find attractors based on recurrences in state space \cite{datseris2022effortless}, from an idea by Nusse and Yorke \cite{nusse1994dynamics}. I then contributed to implementing and refining another algorithm, proposed in Refs. \cite{stender2021bstab, gelbrecht2020monte}, based on finding attractors by grouping trajectories with similar features. These algorithms are described more in \secref{method:sec:finding-atts}. Together with Datseris and Wagemakers, we built a continuation framework that allows one to use either of these two methods across a parameter range. This idea is similar to linear continuation analysis, but generalizes to any type of attractor, including chaotic attractors. This led to a joint publication \cite{datseris2023framework}. On top of the novelty of the continuation algorithm, and the improvements made to the state-of-the-art algorithms for finding attractors, our contribution here was also to provide a package that is free and easy to use. 

Going back now to the excitable neurons, the multistability seen there is remarkable: stable states arise from the interaction with transient behavior (the excitations). Often in the literature we are preoccupied with the final states of the system - usually justifiably so - but anyone who asks the falling hikers in our initial example will probably find out that transients should not be disregarded so easily. In particular for neuroscience, transient dynamics has been the object of a lot of recent work. For instance, transients can be harnessed to perform computations \cite{budzinski2023an}, particularly when they are long-lived \cite{koch2024biological}. Ref. \cite{koch2024biological} proposes that long-lived transients, particularly in the form of ghosts of saddle-node bifurcations, offer some distinct computational advantages, such as maintaining a dynamical memory of a signal. See \secref{method:bifurcations} for more on ghosts. For instance, Ref.~\cite{nandan2022cells} studied a simple model for how cells respond to changing chemical signals and use them to move. Without any signal, the cell operates on a stable equilibrium. A signal causes a saddle-node bifurcation that leads it to another stable equilibrium. As the signal is removed, the inverse bifurcation happens, and the cell eventually converges back to the original equilibrium. But before returning, the cell stays for a while visiting the ghost of the second equilibrium. Biologically, this means that cell keeps the memory of the signal for a while \cite{nandan2022cells, koch2024biological}. 
Indeed, long-lived transients are a ubiquitous phenomenon observed in neural activity \cite{tognoli2014metastable, brinkman2022metastable}, and are often referred to as \textit{metastable}. One interesting example comes from studies measuring how mice encode for tastants fed to them. The study measured the firing rate activity in the gustatory cortex of the mice as a response to different tastants \cite{jones2007natural}. They identified that the stimulus elicits a sequence of distinct long-lived but transient regimes. By regime here we mean an epoch of the time series with some unique properties - in their case, the configuration of the average firing rate across the ensemble of neurons. Each tastant evoked a specific sequence of such metastable regimes. The duration of these regimes varies across trials, but the sequence itself is consistent \cite{lacamera2019cortical, brinkman2022metastable}.

Delving into the metastability literature, we found that a general conceptual framework in neuroscience was lacking. First, the very definition of metastability varied between works, leading to apparent inconsistencies, as explained in more details in \chapref{chap:metastability}. Second, the mechanisms proposed for metastability also varied. Some works propose ghost of saddle-node bifurcations \cite{tognoli2014metastable} while others propose noise \cite{brinkman2022metastable}, with few works attempting to compare different proposals \cite{graben2019metastable}. In our work, we drew from tools of dynamical systems theory to provide such a conceptual framework. We provide a simple definition of metastable regimes as long-lived transients, which encompasses the majority of previous works not only in neuroscience, but also dynamical systems and even ecology. Previous inconsistencies between works can be neatly fit into distinct subtypes of metastability - for instance, when transitions between metastable regimes are spontaneously or externally driven.
% Then we use this definition to study general properties of metastability, making use of the concept of almost-invariant sets \cite{dellnitz2003congestion, froyland2005statistically}. We argue that metastable regimes in time correspond in state space to almost-invariant sets, regions in which trajectories tend to stay for long, but not infinitely long. We also propose several dynamical mechanisms that can generate metastable regimes. Interestingly, we also describe cases in which transient behavior lives inside an attractor - for instance, in a stable heteroclinic cycle, which consists of unstable equilibrium points that are connected in space. Importantly, we connect these dynamical mechanisms to previous literature in neuroscience, complementing the discussions there.   
Then we use this definition to study general properties of metastability, making use of the concept of almost-invariant sets \cite{dellnitz2003congestion, froyland2005statistically}. We argue that metastable regimes in time correspond in state space to almost-invariant sets, regions in which trajectories tend to stay for long, but not infinitely long. We also propose several dynamical mechanisms that can generate metastable regimes. Importantly, we connect these dynamical mechanisms to previous literature in neuroscience, complementing the discussions there.   

Taking all of this together, my PhD has been a journey into studying the long-term and the transient dynamics of networked systems - how multistability can emerge and how it affects their robustness - and how long transients (metastability) can arise.  This thesis describes this journey and will hopefully reflect the excitement of doing all of this research. In \chapref{chap:methodology} I introduce in greater depth the fundamental concepts needed for the studies performed in this thesis. These will then follow in Chaps. \ref{chap:malleability}, \ref{chap:multistability} and \ref{chap:metastability} in the same order introduced here. Finally, in \chapref{chap:conclusions} I will take all of these results together and reflect on what we learned, what our contributions have been to the literature, and the open questions that lie ahead in the future. 


